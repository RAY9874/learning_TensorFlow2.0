{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tensorflow 数据格式处理大全\n",
    " Tensorflow入门难，难在整个计算过程中充斥着让人仅仅知道形式，而无法看到具体内容的图，比如tensor，之前只有在run的时候才能查看，非常不方便。TF提供了很多api，从读取目录到解析图片，只是通过tf框架读入的内容，往往都是tensor形式存储在内存中，很难查看，也无法得知操作是否正确，很难查找问题。\n",
    " \n",
    "所幸新版本TF，提供了tf.enable_eager_execution()模式，可能极大方便我们实时观察tf中数据的变化，感觉有点借鉴了pytorch的思想。\n",
    "\n",
    "本说明基于最新的tf 1.13版本，应该也兼容2.0版本。注意该版本需要将cuda更新到10.0，cuda最新版是10.1，注意不要装错了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 首先导入需要的环境包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.VERSION\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Tensor 与 numpy\n",
    " ### 1.1 基于两个框架的操作的转换\n",
    " TensorFlow operations 会自动将 NumPy ndarrays 转化为 Tensors。\n",
    " 同样，Numpy operations 会自动将 Tensors 转化为 numpy ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]], shape=(3, 3), dtype=float64)\n",
      "[[43. 43. 43.]\n",
      " [43. 43. 43.]\n",
      " [43. 43. 43.]]\n"
     ]
    }
   ],
   "source": [
    "ndarray = np.ones([3, 3])\n",
    "tensor = tf.multiply(ndarray, 42)\n",
    "print(tensor)\n",
    "np_arr = np.add(tensor, 1)\n",
    "print(np_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 如果使用正常的加减乘除也是可以的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "tf.Tensor(\n",
      "[[43. 43. 43.]\n",
      " [43. 43. 43.]\n",
      " [43. 43. 43.]], shape=(3, 3), dtype=float64)\n",
      "[[1806. 1806. 1806.]\n",
      " [1806. 1806. 1806.]\n",
      " [1806. 1806. 1806.]]\n",
      "tf.Tensor(\n",
      "[[1764. 1764. 1764.]\n",
      " [1764. 1764. 1764.]\n",
      " [1764. 1764. 1764.]], shape=(3, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(ndarray + [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "print(tensor + [[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "print(np_arr * 42)\n",
    "print(tensor * 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.2 numpy -> tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[43. 43. 43.]\n",
      " [43. 43. 43.]\n",
      " [43. 43. 43.]], shape=(3, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.convert_to_tensor(np_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1.3 tensor -> numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42. 42. 42.]\n",
      " [42. 42. 42.]\n",
      " [42. 42. 42.]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset与numpy & tensors\n",
    "### 2.1 numpy(tensors) -> Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.int32>\n",
      "<DatasetV1Adapter shapes: (3,), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "ds_tensors1 = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])\n",
    "ds_tensors2 = tf.data.Dataset.from_tensor_slices(np.ones([3,3]))\n",
    "print(ds_tensors1)\n",
    "print(ds_tensors2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，Dataset在显示shape的时候，并不显示第一维，第一维默认是batch。\n",
    "如果存在多个变量，可以采用下面的方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((4, 4), (1,)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "features = np.ones([100, 4, 4])\n",
    "labels = np.ones([100, 1])\n",
    "ds_tensor3 = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(ds_tensor3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，Dataset的大小变为(4, 4), (1, )\n",
    "如果变量较多，我们还可以对其进行命名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: {a: (4, 4), b: (1,)}, types: {a: tf.float64, b: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "ds_tensor4 = tf.data.Dataset.from_tensor_slices({\"a\": features, \"b\": labels})\n",
    "print(ds_tensor4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "上面的代码段会将 features 和 labels 数组作为 tf.constant() 指令嵌入在TensorFlow 图中。这样非常适合小型数据集，但会浪费内存，因为会多次复制数组的内容，并可能会达到 tf.GraphDef 协议缓冲区的 2GB 上限。作为替代方案，可以根据 tf.placeholder() 张量定义 Dataset，并在对数据集初始化 Iterator 时馈送 NumPy 数组。这种情况适用于动态修改Dataset内数据的形式。\n",
    "features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "iterator = dataset.make_initializable_iterator() \n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: features,labels_placeholder: labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，Dataset还有一个函数from_tensors()，一般不用这个。因为这个函数不会将第一维视为batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {a: (?, 100, 4, 4), b: (?, 100, 1)}, types: {a: tf.float64, b: tf.float64}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: {a: (100, 4, 4), b: (100, 1)}, types: {a: tf.float64, b: tf.float64}>\n"
     ]
    }
   ],
   "source": [
    "ds_tensor5 = tf.data.Dataset.from_tensors({\"a\": features, \"b\": labels})\n",
    "ds_tensor5.batch(4)\n",
    "print(ds_tensor5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### 2.2 Dataset -> Dataset\n",
    " Dataset合并到到Dataset跟numpy直接生成有一点点不一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((4, 4), (1,)), types: (tf.float64, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "features_ds = tf.data.Dataset.from_tensor_slices(features)\n",
    "label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "ds_tensor6 = tf.data.Dataset.zip((features_ds, label_ds))\n",
    "print(ds_tensor6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 取Dataset的子集，当参数数量大于数据集大小时，返回整个dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((4, 4), (1,)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tensor6.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.3 Dataset -> tensors & numpy array\n",
    " 当一部分数据转换成Dataset之后，我们想观察一下这个数据集里有哪些，希望转换回来。\n",
    " Dataset标准做法是使用tf.data.Iterator，但是也可以使用python自己的迭代器实现，当且仅当eager execution开启的时候。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\users\\mic\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]] [1.]\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]] [1.]\n"
     ]
    }
   ],
   "source": [
    "features = np.ones([10, 2, 5])\n",
    "labels = np.ones([10, 1])\n",
    "ds_tensor7 = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "for f, l in ds_tensor7.take(2):\n",
    "    #print(f, l) # 输出tensor\n",
    "    print(f.numpy(), l.numpy())  # 输出numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果设置了batch，则会按照batch大小输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]] [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]] [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "ds_tensor8 = ds_tensor7.batch(4)\n",
    "for f, l in ds_tensor8.take(2):\n",
    "    #print(f, l)\n",
    "    print(f.numpy(), l.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想取出整个数据集，可以将batch设置的比数据集更大的一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]] [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "ds_tensor9 = ds_tensor7.batch(100, drop_remainder=False)\n",
    "for f, l in ds_tensor9:\n",
    "    #print(f, l)\n",
    "    print(f.numpy(), l.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种方法，使用python的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=83, shape=(2, 5), dtype=float64, numpy=\n",
       " array([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])>,\n",
       " <tf.Tensor: id=84, shape=(1,), dtype=float64, numpy=array([1.])>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration = iter(ds_tensor7)\n",
    "next(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有开启eager execution，使用起来会麻烦一些，分为单次迭代、可初始化、可重新初始化，以及可馈送。具体可参考https: // www.tensorflow.org/guide/datasets#creating_an_iterator\n",
    "感觉用起来挺麻烦的，待总结需求情况。\n",
    "下面是常规使用方法\n",
    "\n",
    "dataset = tf.data.Dataset.range(5)\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "返回的next_element是tensor。既然2.0默认开启eager模式，这种方式就暂时不考虑了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 更加复杂的Dataset构成\n",
    "对于复杂的datset，需要先对每一个变量进行构建，然后再通过zip方式拼接在一起。\n",
    "\n",
    "对于直接混合方式的数据，tf是无法处理的。例如，在list中存储一组dict，或者list中包含多个矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((5, 5), (4, 4)), types: (tf.float64, tf.float64)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (((5, 5), (4, 4)), ((), ())), types: ((tf.float64, tf.float64), (tf.string, tf.int32))>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({im1: (5, 5)}, {im2: (4, 4)}), types: ({im1: tf.float64}, {im2: tf.float64})>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({lb1: ()}, {lb2: ()}), types: ({lb1: tf.string}, {lb2: tf.int32})>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (({im1: (5, 5)}, {im2: (4, 4)}), ({lb1: ()}, {lb2: ()})), types: (({im1: tf.float64}, {im2: tf.float64}), ({lb1: tf.string}, {lb2: tf.int32}))>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {inputs: ({im1: (5, 5)}, {im2: (4, 4)}), outputs: ({lb1: ()}, {lb2: ()})}, types: {inputs: ({im1: tf.float64}, {im2: tf.float64}), outputs: ({lb1: tf.string}, {lb2: tf.int32})}>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_observations = int(1e2)\n",
    "\n",
    "# multi input and output\n",
    "img1 = np.arange(n_observations*25).reshape([n_observations, 5, 5]).astype(np.float64)\n",
    "img2 = np.arange(n_observations*16).reshape([n_observations, 4, 4]).astype(np.float64)\n",
    "img1 = tf.data.Dataset.from_tensor_slices(img1)\n",
    "img2 = tf.data.Dataset.from_tensor_slices(img2)\n",
    "img = tf.data.Dataset.zip((img1, img2))\n",
    "img\n",
    "label1 = [\"cat\"]*n_observations\n",
    "label2 = [0]*n_observations\n",
    "label1 = tf.data.Dataset.from_tensor_slices(label1)\n",
    "label2 = tf.data.Dataset.from_tensor_slices(label2)\n",
    "label = tf.data.Dataset.zip((label1, label2))\n",
    "label\n",
    "ds = tf.data.Dataset.zip((img, label))\n",
    "ds\n",
    "\n",
    "# 带名称的字典形式 multi input and output\n",
    "img1 = np.arange(n_observations*25).reshape([n_observations, 5, 5]).astype(np.float64)\n",
    "img2 = np.arange(n_observations*16).reshape([n_observations, 4, 4]).astype(np.float64)\n",
    "img1 = tf.data.Dataset.from_tensor_slices({\"im1\": img1})\n",
    "img2 = tf.data.Dataset.from_tensor_slices({\"im2\": img2})\n",
    "img = tf.data.Dataset.zip((img1, img2))\n",
    "img\n",
    "label1 = [\"cat\"]*n_observations\n",
    "label2 = [0]*n_observations\n",
    "label1 = tf.data.Dataset.from_tensor_slices({\"lb1\":label1})\n",
    "label2 = tf.data.Dataset.from_tensor_slices({\"lb2\":label2})\n",
    "label = tf.data.Dataset.zip((label1, label2))\n",
    "label\n",
    "ds = tf.data.Dataset.zip((img, label))\n",
    "ds\n",
    "def change_format(x1, x2):\n",
    "    return {\"inputs\":x1, \"outputs\":x2}\n",
    "ds = ds.map(change_format)\n",
    "ds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Dataset 与 TFRecord\n",
    " 与TFRecord打交道，先把下面三个函数加上，分别处理字符、浮点型和整数型（包括bool型）。\n",
    " TFRecord之所以这样设计，是因为在数据读取时，如果每个数据都是一个连续序列的话，读取速度会快很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 创建要存储的数据\n",
    " the number of observations in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = int(1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 4.1 5x5 image float数据读写\n",
    " 使用numpy生成5x5的float类型数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (5, 5), types: tf.float64>\n",
      "<DatasetV1Adapter shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "feature1 = np.arange(n_observations*25).reshape([n_observations, 5, 5]).astype(np.float64)\n",
    "feature1 = tf.data.Dataset.from_tensor_slices(feature1)\n",
    "print(feature1)\n",
    "#map函数的参数为一个函数，可以分别单独处理dataset中的每一个元素\n",
    "restore_shape= feature1.output_shapes\n",
    "feature1 = feature1.map(tf.serialize_tensor)\n",
    "print(feature1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写入TFRecord文档中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test_float_matrix.tfrec\"\n",
    "tfrec = tf.data.experimental.TFRecordWriter(filename)\n",
    "tfrec.write(feature1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取存储的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (5, 5), types: tf.float64>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[25. 26. 27. 28. 29.]\n",
      " [30. 31. 32. 33. 34.]\n",
      " [35. 36. 37. 38. 39.]\n",
      " [40. 41. 42. 43. 44.]\n",
      " [45. 46. 47. 48. 49.]], shape=(5, 5), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def parse(x):\n",
    "    result = tf.parse_tensor(x, out_type=tf.float64)  # 注意查看存储时候的数据类型需要一致\n",
    "    result = tf.reshape(result, tf.TensorShape([5,5])) # 不进行此操作，shape会显示为<unknown>，影响后续操作\n",
    "    return result\n",
    "ds2 = tf.data.TFRecordDataset(filename).map(parse)\n",
    "ds2\n",
    "for x in ds2.take(2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 字符串 string feature 数据读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "random_choice = np.random.randint(0, 5, n_observations).astype(np.int64)\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[random_choice]\n",
    "feature2 = tf.data.Dataset.from_tensor_slices(feature2)\n",
    "print(feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'chicken', shape=(), dtype=string)\n",
      "tf.Tensor(b'cat', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_string.tfrec\"\n",
    "tfrec = tf.data.experimental.TFRecordWriter(filename)\n",
    "tfrec.write(feature2)\n",
    "\n",
    "ds2 = tf.data.TFRecordDataset(filename)\n",
    "\n",
    "for x in ds2.take(2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 1x1维数字数据读写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: (), types: tf.int64>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_choice = np.random.randint(0, 5, n_observations).astype(np.int64)\n",
    "feature3 = tf.data.Dataset.from_tensor_slices(random_choice)\n",
    "print(feature3)\n",
    "feature3 = feature3.map(tf.serialize_tensor)\n",
    "feature3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "filename = \"test_int.tfrec\"\n",
    "tfrec = tf.data.experimental.TFRecordWriter(filename)\n",
    "tfrec.write(feature3)\n",
    "def parse(x):\n",
    "    result = tf.parse_tensor(x, out_type=tf.int64)  # 注意查看存储时候的数据类型需要一致\n",
    "    return result\n",
    "ds3 = tf.data.TFRecordDataset(filename).map(parse)\n",
    "\n",
    "for x in ds3.take(2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 多个特征同时写入\n",
    "在写入多个特征时，通常用于数据和标签一一对应存储的情况，这时需要用到tf.example。注意，对于每一个包含多个项目的数据（下面统称为一个example），如果这个项目是单个值，可以使用下面的函数；如果是数组或矩阵，要先转化为字符串再进行存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int32_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int32_list=tf.train.Int32List(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义字符串化每一个example的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((), (), ()), types: (tf.string, tf.string, tf.int64)>\n",
      "WARNING:tensorflow:From <ipython-input-23-bf1247de8473>:22: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "<DatasetV1Adapter shapes: (), types: tf.string>\n"
     ]
    }
   ],
   "source": [
    "# 单独每一个序列化\n",
    "def serialize_example(feature1, feature2, feature3):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "    feature = {\n",
    "        'feature1': _bytes_feature(feature1),\n",
    "        'feature2': _bytes_feature(feature2),\n",
    "        'feature3': _int64_feature(feature3),\n",
    "    }\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "# example 序列化\n",
    "def tf_serialize_example(f1,f2,f3):\n",
    "    tf_string = tf.py_func( # 2.0好像会使用tf.py_function，但是在1.13版本中，如果替换成tf.py_function会出错\n",
    "      serialize_example, \n",
    "      (f1,f2,f3),  # pass these args to the above function.\n",
    "      tf.string)      # the return type is <a href=\"../../api_docs/python/tf#string\"><code>tf.string</code></a>.\n",
    "    return tf.reshape(tf_string, ())\n",
    "feature3 = np.random.randint(0, 5, n_observations).astype(np.int64)\n",
    "feature3 = tf.data.Dataset.from_tensor_slices(feature3)\n",
    "multi_ds = tf.data.Dataset.zip((feature1, feature2, feature3))\n",
    "print(multi_ds)\n",
    "multi_ds2 = multi_ds.map(tf_serialize_example)\n",
    "print(multi_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写入TFRecord文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'test_multi_data.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(multi_ds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入TFRecord文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV1 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {feature1: (), feature2: (), feature3: ()}, types: {feature1: tf.string, feature2: tf.string, feature3: tf.int64}>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入时需要知道原先都有哪些格式\n",
    "feature_description = {\n",
    "    'feature1': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature2': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3': tf.FixedLenFeature([], tf.int64, default_value=0)\n",
    "}\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    return tf.parse_single_example(example_proto, feature_description)\n",
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解析feature1的float矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((5, 5), (), ()), types: (tf.float64, tf.string, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float64)\n",
      "tf.Tensor(b'chicken', shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[25. 26. 27. 28. 29.]\n",
      " [30. 31. 32. 33. 34.]\n",
      " [35. 36. 37. 38. 39.]\n",
      " [40. 41. 42. 43. 44.]\n",
      " [45. 46. 47. 48. 49.]], shape=(5, 5), dtype=float64)\n",
      "tf.Tensor(b'cat', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "def _parse_feature1(dictf):\n",
    "    res = tf.parse_tensor(dictf['feature1'], out_type=tf.float64)\n",
    "    res = tf.reshape(res, tf.TensorShape([5,5]))\n",
    "    return res,dictf['feature2'],dictf['feature3']\n",
    "parsed_dataset2 = parsed_dataset.map(_parse_feature1)\n",
    "parsed_dataset2\n",
    "for a,b,c in parsed_dataset2.take(2):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实际使用pipline形式\n",
    "参考上面的内容，应当很容易看到官网教程中https://www.tensorflow.org/tutorials/load_data/images#build_a_tfdatadataset 的描述。\n",
    "下面是伪代码：\n",
    "\n",
    "已知有两个list分别包含，all_image_paths、all_label_paths,这个可通过外部维护"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_image_paths=[]\n",
    "# all_label_paths=[]\n",
    "# def read_image(path):\n",
    "#     # 返回读取的image\n",
    "#     pass\n",
    "# def read_label(path):\n",
    "#     # 返回读取的label\n",
    "#     pass\n",
    "# def load_and_preprocess_from_path_label(img_path, label_path):\n",
    "#       return read_image(img_path), read_label(label_path)\n",
    "\n",
    "# ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_label_paths))#这个比较小，可以都读入内存\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# ds = ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BATCH_SIZE = 32\n",
    "# image_count = len(all_image_paths)\n",
    "# # 将随机打乱的buffer设置到跟数据集一样大，这样能保证数据成分随机化\n",
    "# ds = ds.shuffle(buffer_size=image_count)\n",
    "# # 先打乱再repeat可以保证不同epoch之间数据不重复，这里repeat可以设置int参数，表示具体重复多少次；为空代表无限循环。\n",
    "# ds = ds.repeat()\n",
    "# ds = ds.batch(BATCH_SIZE)\n",
    "# # `prefetch` 可以让dataset在训练过程中，在后台读取batch\n",
    "# ds = ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个网址中，讲到了如何在keras中使用dataset https://tensorflow.google.cn/guide/keras#input_tfdata_datasets\n",
    "\n",
    "在一般模型中，dataset迭代生成的结果本身就是tensor，可以直接输入到模型中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Keras 模型示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 28, 28), (?,)), types: (tf.float64, tf.int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_NUM = 1000\n",
    "x_train = np.random.normal(loc=0, scale=1, size=[IMG_NUM, 28, 28])\n",
    "y_train = np.random.choice(10, size=IMG_NUM)\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# 将随机打乱的buffer设置到跟数据集一样大，这样能保证数据成分随机化\n",
    "ds = ds.shuffle(buffer_size=1000)\n",
    "# 先打乱再repeat可以保证不同epoch之间数据不重复，这里repeat可以设置int参数，表示具体重复多少次；为空代表无限循环。\n",
    "ds = ds.repeat()\n",
    "ds = ds.batch(BATCH_SIZE)\n",
    "# `prefetch` 可以让dataset在训练过程中，在后台读取batch\n",
    "ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\users\\mic\\anaconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 3s 102ms/step - loss: 2.8052 - acc: 0.1104\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.7812 - acc: 0.8340\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.2513 - acc: 0.9912\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1111 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 7ms/step - loss: 0.0650 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26bcc3e59e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "steps_per_epoch=tf.ceil(IMG_NUM/BATCH_SIZE).numpy().astype(np.int32)\n",
    "model.fit(ds, epochs=5, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2.7633 - acc: 0.1064\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.7898 - acc: 0.8408\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2380 - acc: 0.9961\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1084 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0689 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26bcd85d630>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(28,28))  # Returns a placeholder tensor\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "predictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "model2 = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "steps_per_epoch=tf.ceil(IMG_NUM/BATCH_SIZE).numpy().astype(np.int32)\n",
    "model2.fit(ds, epochs=5, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tensorflow 模型示例 非eager excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.random.normal(loc=0, scale=1, size=[IMG_NUM, 28, 28])\n",
    "# y_train = np.random.choice(10, size=IMG_NUM)\n",
    "# y_train = tf.one_hot(y_train, 10)\n",
    "# ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# ds.batch(32)\n",
    "# inputs = tf.placeholder(tf.float64, shape=[None, 28, 28])\n",
    "# outputs = tf.placeholder(tf.int32, shape=[None, 10])\n",
    "# def model3(inputs, outputs):\n",
    "#     x = tf.layers.Flatten()(inputs)\n",
    "#     x = tf.layers.Dense(512, activation='relu')(x)\n",
    "#     x = tf.layers.Dropout(0.2)(x)\n",
    "#     pred = tf.layers.Dense(10, activation='softmax')(x)\n",
    "#     loss = tf.losses.softmax_cross_entropy(outputs, pred)\n",
    "#     train = tf.train.AdamOptimizer().minimize(loss)\n",
    "#     return pred, loss, train\n",
    "# p, l, t = model3(inputs, outputs)\n",
    "\n",
    "\n",
    "# sess = tf.Session()\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess.run(init)\n",
    "\n",
    "# iterator = ds.make_one_shot_iterator()\n",
    "# next_element = iterator.get_next()\n",
    "\n",
    "# for i in range(1000):\n",
    "#     in_var, out_var = sess.run(next_element)\n",
    "#     sess.run(t, feed_dict={inputs:in_var, outputs:out_var})\n",
    "#     print(loss.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}